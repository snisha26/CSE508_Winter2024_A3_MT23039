{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8053928,"sourceType":"datasetVersion","datasetId":4749967},{"sourceId":8056491,"sourceType":"datasetVersion","datasetId":4751775},{"sourceId":8056626,"sourceType":"datasetVersion","datasetId":4751877},{"sourceId":8057575,"sourceType":"datasetVersion","datasetId":4752551},{"sourceId":8061907,"sourceType":"datasetVersion","datasetId":4755665},{"sourceId":8230849,"sourceType":"datasetVersion","datasetId":4758652},{"sourceId":8094780,"sourceType":"datasetVersion","datasetId":4758200}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Function to filter products belonging to both \"Electronics\" and \"Headphones\" categories\ndef is_headphones(data):\n    categories = \"Electronics\" in data.get('category', []) and \"Headphones\" in data.get('category', [])\n    return categories\n\n# Function to read metadata from meta file\ndef read_metadata(file_path):\n    metadata_dict = {}\n    with open(file_path, 'r') as f:\n        for line in f:\n            try:\n                data = json.loads(line)\n                if is_headphones(data):\n                    metadata_dict[data['asin']] = data\n            except json.JSONDecodeError as e:\n                print(f\"Error decoding JSON: {e}\")\n    return metadata_dict\n\n# Function to extract reviews from review file\ndef extract_reviews(file_path, metadata_dict):\n    reviews = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            try:\n                data = json.loads(line)\n                asin = data.get('asin')\n                if asin in metadata_dict:\n                    reviews.append(data)\n            except json.JSONDecodeError as e:\n                print(f\"Error decoding JSON: {e}\")\n    return reviews\n\n# Read metadata\nmetadata_dict = read_metadata('/kaggle/input/electronics/meta_Electronics.json')\n\n# Extract reviews for headphones\nheadphones_reviews = extract_reviews('/kaggle/input/electronics/Electronics_5.json', metadata_dict)\n\n# Convert reviews to DataFrame\nreviews_df = pd.DataFrame(headphones_reviews)\n\n# Convert metadata to DataFrame\nmetadata_df = pd.DataFrame(metadata_dict.values())\n\nprint(metadata_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata_df.to_csv('headphones_metadata.csv', index=False, escapechar='\\\\')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.to_csv('headphones_reviews.csv', index=False, escapechar='\\\\')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(reviews_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the headphones reviews CSV file\nreviews_df = pd.read_csv('/kaggle/input/after-extraction-files/headphones_reviews.csv')\n\n# Report the total number of rows before preprocessing\ntotal_rows = len(reviews_df)\nprint(\"Total number of rows before preprocessing:\", total_rows)\n\n# Handling missing values\nreviews_df.dropna(inplace=True)\n\n# Removing duplicates\nreviews_df.drop_duplicates(inplace=True)\n\n# Report the new total number of rows after preprocessing\nnew_total_rows = len(reviews_df)\nprint(\"Total number of rows after preprocessing:\", new_total_rows)\n\n# Save the preprocessed headphones reviews to a new CSV file\nreviews_df.to_csv('preprocessed_headphones_reviews.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:48:37.451791Z","iopub.execute_input":"2024-04-25T18:48:37.452473Z","iopub.status.idle":"2024-04-25T18:48:45.406127Z","shell.execute_reply.started":"2024-04-25T18:48:37.452437Z","shell.execute_reply":"2024-04-25T18:48:45.405054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(reviews_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:49:50.786583Z","iopub.execute_input":"2024-04-25T18:49:50.786913Z","iopub.status.idle":"2024-04-25T18:49:50.803008Z","shell.execute_reply.started":"2024-04-25T18:49:50.786888Z","shell.execute_reply":"2024-04-25T18:49:50.802090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the preprocessed headphones reviews CSV file\nreviews_df = pd.read_csv('/kaggle/input/preprocessed-reviews/preprocessed_headphones_reviews.csv')\n\n# a. Number of Reviews\nnum_reviews = len(reviews_df)\n\n# b. Average Rating Score\navg_rating_score = reviews_df['overall'].mean()\n\n# c. Number of Unique Products\nnum_unique_products = len(reviews_df['asin'].unique())\n\n# d. Number of Good Rating (ratings >= 3)\nnum_good_ratings = len(reviews_df[reviews_df['overall'] >= 3])\n\n# e. Number of Bad Ratings (ratings < 3)\nnum_bad_ratings = len(reviews_df[reviews_df['overall'] < 3])\n\n# f. Number of Reviews corresponding to each Rating\nreviews_per_rating = reviews_df['overall'].value_counts().sort_index()\n\n# Print the descriptive statistics\nprint(\"Number of Reviews:\", num_reviews)\nprint(\"Average Rating Score:\", avg_rating_score)\nprint(\"Number of Unique Products:\", num_unique_products)\nprint(\"Number of Good Ratings:\", num_good_ratings)\nprint(\"Number of Bad Ratings:\", num_bad_ratings)\nprint(\"\\nNumber of Reviews corresponding to each Rating:\")\nprint(reviews_per_rating)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:49:56.490867Z","iopub.execute_input":"2024-04-25T18:49:56.491571Z","iopub.status.idle":"2024-04-25T18:49:56.651971Z","shell.execute_reply.started":"2024-04-25T18:49:56.491515Z","shell.execute_reply":"2024-04-25T18:49:56.651078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re\n\n# Load the English language model in spaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load the preprocessed headphones reviews CSV file\nreviews_df = pd.read_csv('/kaggle/input/preprocessed-reviews/preprocessed_headphones_reviews.csv')\n\n# Define the acronyms and their expansions\nacronyms = { 'ANC': 'Active Noise Cancellation',\n             'BT': 'Bluetooth',\n             'IEM': 'In-Ear Monitor',\n             'OEM': 'Original Equipment Manufacturer',\n             'DAC': 'Digital-to-Analog Converter',\n             'AMP': 'Amplifier',\n             'EQ': 'Equalizer',\n             'DAP': 'Digital Audio Player',\n             'LDAC': 'Lossless Digital Audio Codec',\n             'AAC': 'Advanced Audio Coding',\n             'aptX': 'Audio Codec developed by Qualcomm',\n             'THD': 'Total Harmonic Distortion',\n             'SPL': 'Sound Pressure Level',\n             'RMS': 'Root Mean Square',\n             'Hz': 'Hertz (frequency measurement)',\n             'dB': 'Decibel (sound pressure level measurement)',\n             'CVC': 'Clear Voice Capture (noise reduction technology)',\n             'NC': 'Noise Cancelling',\n             'L/R': 'Left/Right (channel designation)',\n             'TRS': 'Tip-Ring-Sleeve (audio connector type)',\n             'TRRS': 'Tip-Ring-Ring-Sleeve (audio connector type, commonly used for headphones with built-in microphones)',\n             'MMCX': 'Micro Miniature Coaxial Connector (used in some detachable cable systems for headphones)',\n             'THX': 'Quality Assurance and Certification company for sound and visual reproduction',\n             'VSS': 'Virtual Surround Sound',\n             'DSD': 'Direct-Stream Digital (audio format)',\n             'XLR': 'A type of electrical connector often used in professional audio equipment',\n             'LFE': 'Low-Frequency Effects (dedicated channel for low-frequency audio in surround sound systems)',\n             'DSP': 'Digital Signal Processing',\n             'TWS': 'True Wireless Stereo (wireless earbuds that don\\'t require a physical connection between them)',\n             'IEMs': 'In-Ear Monitors'\n}\n\n# Remove HTML Tags\ndef remove_html_tags(text):\n    try:\n        soup = BeautifulSoup(text, \"html.parser\")\n        return soup.get_text()\n    except Exception as e:\n        print(\"Error occurred during HTML tag removal:\", e)\n        return text\n\nreviews_df['reviewText'] = reviews_df['reviewText'].apply(remove_html_tags)\n\n# Expanding Acronyms\ndef expand_acronyms(text):\n    try:\n        for acronym, expansion in acronyms.items():\n            text = re.sub(r'\\b' + re.escape(acronym) + r'\\b', expansion, text)\n        return text\n    except Exception as e:\n        print(\"Error occurred during acronym expansion:\", e)\n        return text\n\nreviews_df['reviewText'] = reviews_df['reviewText'].apply(expand_acronyms)\n\n# Removing Special Characters\ndef remove_special_characters(text):\n    try:\n        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n        return text\n    except Exception as e:\n        print(\"Error occurred during special character removal:\", e)\n        return text\n\nreviews_df['reviewText'] = reviews_df['reviewText'].apply(remove_special_characters)\n\n# Lemmatization using spaCy\ndef lemmatize_text(text):\n    try:\n        doc = nlp(text)\n        lemmatized_text = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n        return ' '.join(lemmatized_text)\n    except Exception as e:\n        print(\"Error occurred during lemmatization:\", e)\n        return text\n\nreviews_df['reviewText'] = reviews_df['reviewText'].apply(lemmatize_text)\n\n# Text Normalization (Lowercasing and Removing Stopwords)\ndef text_normalization(text):\n    try:\n        tokens = text.lower().split()\n        normalized_text = [token for token in tokens if token not in ENGLISH_STOP_WORDS and token.isalpha()]\n        return ' '.join(normalized_text)\n    except Exception as e:\n        print(\"Error occurred during text normalization:\", e)\n        return text\n\nreviews_df['reviewText'] = reviews_df['reviewText'].apply(text_normalization)\n\n# Print the preprocessed text\nprint(reviews_df['reviewText'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:50:01.121089Z","iopub.execute_input":"2024-04-25T18:50:01.121774Z","iopub.status.idle":"2024-04-25T18:52:08.403964Z","shell.execute_reply.started":"2024-04-25T18:50:01.121741Z","shell.execute_reply":"2024-04-25T18:52:08.402927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.to_csv('preprocessed_headphones_reviewsText.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:00:20.869309Z","iopub.execute_input":"2024-04-25T19:00:20.870080Z","iopub.status.idle":"2024-04-25T19:00:21.004021Z","shell.execute_reply.started":"2024-04-25T19:00:20.870047Z","shell.execute_reply":"2024-04-25T19:00:21.003043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\n# Load the dataset\nreviews_df = pd.read_csv('/kaggle/input/preprocessed-reviewtext/preprocessed_headphones_reviewsText.csv')\n\n# Task a: Top 20 most reviewed brands\ntop_20_most_reviewed_brands = reviews_df['asin'].value_counts().head(20)\n#brands= list(reviews_df.groupby('brand').count().sort_values(by='reviewText',ascending=False)['reviewText'].index)\n#tom = pd.dataframe({'brand name':brands[:20],'review_count':count[:20]})\nprint(\"Task a: Top 20 most reviewed brands:\\n\", top_20_most_reviewed_brands)\n#print(\"Task a: Top 20 most reviewed brands:\\n\", tom)\n\n# Task b: Top 20 least reviewed brands\ntop_20_least_reviewed_brands = reviews_df['asin'].value_counts().tail(20)\nprint(\"\\nTask b: Top 20 least reviewed brands:\\n\", top_20_least_reviewed_brands)\n#tol = pd.dataframe({'brand name':brands[:20],'review_count':count[:20]})\n\n# Task c: Most positively reviewed headphone\nmost_positively_reviewed_headphone = reviews_df[reviews_df['overall'] == 5]['asin'].value_counts().idxmax()\nprint(\"\\nTask c: Most positively reviewed headphone:\", most_positively_reviewed_headphone)\n\n# Task d: Show the count of ratings for the product over 5 consecutive years\nreviews_df['reviewTime'] = pd.to_datetime(reviews_df['reviewTime'])\nreviews_df['year'] = reviews_df['reviewTime'].dt.year\nratings_count_over_years = reviews_df.groupby(['asin', 'year'])['overall'].count().unstack().fillna(0).astype(int)\nprint(\"\\nTask d: Count of ratings for the product over 5 consecutive years:\\n\", ratings_count_over_years)\n\n# Task e: Word Cloud for 'Good' and 'Bad' ratings\ngood_reviews_text = ' '.join(reviews_df[reviews_df['overall'] == 5]['reviewText'])\nbad_reviews_text = ' '.join(reviews_df[reviews_df['overall'] < 3]['reviewText'])\ngood_reviews_wordcloud = WordCloud(background_color='white').generate(good_reviews_text)\nbad_reviews_wordcloud = WordCloud(background_color='white').generate(bad_reviews_text)\nprint(\"\\nTask e: Word Cloud for 'Good' and 'Bad' ratings:\")\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(good_reviews_wordcloud, interpolation='bilinear')\nplt.title('Word Cloud for Good Ratings')\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(bad_reviews_wordcloud, interpolation='bilinear')\nplt.title('Word Cloud for Bad Ratings')\nplt.axis('off')\nplt.show()\n\n# Task f: Plot a pie chart for Distribution of Ratings vs. the No. of Reviews\n\nrating_distribution = reviews_df['overall'].value_counts()\nprint(\"\\nTask f: Distribution of Ratings vs. No. of Reviews:\\n\", rating_distribution)\nplt.figure(figsize=(8, 8))\nplt.pie(rating_distribution, labels=rating_distribution.index, autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of Ratings')\nplt.show()\n\n# Task g: Report in which year the product got maximum reviews\nyear_with_max_reviews = reviews_df['year'].value_counts().idxmax()\nprint(\"\\nTask g: Year with maximum reviews:\", year_with_max_reviews)\n\n# Task h: Which year has the highest number of Customers?\nyear_with_highest_customers = reviews_df.groupby('year')['reviewerID'].nunique().idxmax()\nprint(\"\\nTask h: Year with the highest number of Customers:\", year_with_highest_customers)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:00:23.309746Z","iopub.execute_input":"2024-04-25T19:00:23.310099Z","iopub.status.idle":"2024-04-25T19:00:26.649234Z","shell.execute_reply.started":"2024-04-25T19:00:23.310074Z","shell.execute_reply":"2024-04-25T19:00:26.647588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#question 6 corrected\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\n# Load the dataset\nreviews_df = pd.read_csv('/kaggle/input/preprocessed-reviewtext/preprocessed_headphones_reviewsText.csv')\n\n# Task a: Top 20 most reviewed brands\ntop_20_most_reviewed_brands = reviews_df['brand'].value_counts().head(20)\nmost_reviewed_brands = top_20_most_reviewed_brands.index.tolist()\nmost_reviewed_counts = top_20_most_reviewed_brands.values.tolist()\n\ntop_20_most_reviewed_df = pd.DataFrame({'Brand Name': most_reviewed_brands, 'Review Count': most_reviewed_counts})\nprint(\"Task a: Top 20 most reviewed brands:\\n\", top_20_most_reviewed_df)\n\n# Task b: Top 20 least reviewed brands\ntop_20_least_reviewed_brands = reviews_df['brand'].value_counts().tail(20)\nleast_reviewed_brands = top_20_least_reviewed_brands.index.tolist()\nleast_reviewed_counts = top_20_least_reviewed_brands.values.tolist()\n\ntop_20_least_reviewed_df = pd.DataFrame({'Brand Name': least_reviewed_brands, 'Review Count': least_reviewed_counts})\nprint(\"\\nTask b: Top 20 least reviewed brands:\\n\", top_20_least_reviewed_df)\n\n# Task c: Most positively reviewed headphone\nmost_positively_reviewed_headphone = reviews_df[reviews_df['overall'] == 5]['asin'].value_counts().idxmax()\nprint(\"\\nTask c: Most positively reviewed headphone:\", most_positively_reviewed_headphone)\n\n# Task d: Show the count of ratings for the product over 5 consecutive years\nreviews_df['reviewTime'] = pd.to_datetime(reviews_df['reviewTime'])\nreviews_df['year'] = reviews_df['reviewTime'].dt.year\nratings_count_over_years = reviews_df.groupby(['asin', 'year'])['overall'].count().unstack().fillna(0).astype(int)\nprint(\"\\nTask d: Count of ratings for the product over 5 consecutive years:\\n\", ratings_count_over_years)\n\n# Task e: Word Cloud for 'Good' and 'Bad' ratings\ngood_reviews_text = ' '.join(reviews_df[reviews_df['overall'] == 5]['reviewText'])\nbad_reviews_text = ' '.join(reviews_df[reviews_df['overall'] < 3]['reviewText'])\ngood_reviews_wordcloud = WordCloud(background_color='white').generate(good_reviews_text)\nbad_reviews_wordcloud = WordCloud(background_color='white').generate(bad_reviews_text)\nprint(\"\\nTask e: Word Cloud for 'Good' and 'Bad' ratings:\")\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(good_reviews_wordcloud, interpolation='bilinear')\nplt.title('Word Cloud for Good Ratings')\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(bad_reviews_wordcloud, interpolation='bilinear')\nplt.title('Word Cloud for Bad Ratings')\nplt.axis('off')\nplt.show()\n\n# Task f: Plot a pie chart for Distribution of Ratings vs. the No. of Reviews\n\nrating_distribution = reviews_df['overall'].value_counts()\nprint(\"\\nTask f: Distribution of Ratings vs. No. of Reviews:\\n\", rating_distribution)\nplt.figure(figsize=(8, 8))\nplt.pie(rating_distribution, labels=rating_distribution.index, autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of Ratings')\nplt.show()\n\n# Task g: Report in which year the product got maximum reviews\nyear_with_max_reviews = reviews_df['year'].value_counts().idxmax()\nprint(\"\\nTask g: Year with maximum reviews:\", year_with_max_reviews)\n\n# Task h: Which year has the highest number of Customers?\nyear_with_highest_customers = reviews_df.groupby('year')['reviewerID'].nunique().idxmax()\nprint(\"\\nTask h: Year with the highest number of Customers:\", year_with_highest_customers)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:23:14.438096Z","iopub.status.idle":"2024-04-25T23:23:14.438442Z","shell.execute_reply.started":"2024-04-25T23:23:14.438274Z","shell.execute_reply":"2024-04-25T23:23:14.438289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom gensim.models import Word2Vec\n\n# Example data\nreviews = reviews_df['reviewText']\n\n# Bag of Words (BoW)\nvectorizer_bow = CountVectorizer()\nX_bow = vectorizer_bow.fit_transform(reviews)\n\n# TF-IDF\nvectorizer_tfidf = TfidfVectorizer()\nX_tfidf = vectorizer_tfidf.fit_transform(reviews)\n\n# Word2Vec\n# Preprocess the text for Word2Vec training\ntokenized_reviews = [review.split() for review in reviews]\n# Train the Word2Vec model\nmodel_w2v = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n\n# Example usage of Word2Vec to get vector representation of a word\nword_vec = model_w2v.wv['good']\n\n# Example usage of Word2Vec to get vector representation of a sentence/document\ndocument_vec = model_w2v.wv[tokenized_reviews[0]]  # Example using the first document\n\n# Output shapes of the feature matrices\nprint(\"Bag of Words (BoW) feature matrix shape:\", X_bow.shape)\nprint(\"TF-IDF feature matrix shape:\", X_tfidf.shape)\nprint(\"Word2Vec embedding shape (example for a word):\", word_vec.shape)\nprint(\"Word2Vec embedding shape (example for a document):\", document_vec.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:00:34.236898Z","iopub.execute_input":"2024-04-25T19:00:34.237253Z","iopub.status.idle":"2024-04-25T19:00:51.966719Z","shell.execute_reply.started":"2024-04-25T19:00:34.237223Z","shell.execute_reply":"2024-04-25T19:00:51.965807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to categorize ratings\ndef categorize_rating(rating):\n    if rating > 3:\n        return 'Good'\n    elif rating == 3:\n        return 'Average'\n    else:\n        return 'Bad'\n\n# Apply the function to create a new column for rating class\nreviews_df['rating_class'] = reviews_df['overall'].apply(categorize_rating)\n\n# Check the updated DataFrame\nprint(reviews_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:01:02.793940Z","iopub.execute_input":"2024-04-25T19:01:02.794607Z","iopub.status.idle":"2024-04-25T19:01:02.812637Z","shell.execute_reply.started":"2024-04-25T19:01:02.794570Z","shell.execute_reply":"2024-04-25T19:01:02.811571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the review text and corresponding rating class in a CSV file\nreviews_df[['reviewText', 'rating_class']].to_csv('review_text_with_rating_class.csv', index=False)\n\n# Confirmation message\nprint(\"CSV file saved successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:01:09.706868Z","iopub.execute_input":"2024-04-25T19:01:09.707217Z","iopub.status.idle":"2024-04-25T19:01:09.801476Z","shell.execute_reply.started":"2024-04-25T19:01:09.707189Z","shell.execute_reply":"2024-04-25T19:01:09.800586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the CSV file\ndata = pd.read_csv('/kaggle/input/reviewtext-ratingclass/review_text_with_rating_class.csv')\n\n# Split the data into features (X) and target variable (y)\nX = data['reviewText']\ny = data['rating_class']\n\n# Divide the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Confirmation message\nprint(\"Data split into train and test sets successfully.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:01:43.859986Z","iopub.execute_input":"2024-04-25T19:01:43.860941Z","iopub.status.idle":"2024-04-25T19:01:43.891334Z","shell.execute_reply.started":"2024-04-25T19:01:43.860905Z","shell.execute_reply":"2024-04-25T19:01:43.890299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LinearRegression\n\n\n# Vectorize the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Initialize the machine learning models\nsvm = SVC()\nnaive_bayes = MultinomialNB()\ndecision_tree = DecisionTreeClassifier()\nrandom_forest = RandomForestClassifier()\nlinear_regression = LinearRegression()\n\n# Convert target variable to numerical format\ny_train_numeric = y_train.map({'Good': 1, 'Average': 0, 'Bad': -1})\ny_test_numeric = y_test.map({'Good': 1, 'Average': 0, 'Bad': -1})\n\n# Train each model\nsvm.fit(X_train_tfidf, y_train)\nnaive_bayes.fit(X_train_tfidf, y_train)\ndecision_tree.fit(X_train_tfidf, y_train)\nrandom_forest.fit(X_train_tfidf, y_train)\nlinear_regression.fit(X_train_tfidf, y_train_numeric)\n\n# Predict on the test set\nsvm_preds = svm.predict(X_test_tfidf)\nnb_preds = naive_bayes.predict(X_test_tfidf)\ndt_preds = decision_tree.predict(X_test_tfidf)\nrf_preds = random_forest.predict(X_test_tfidf)\nlr_preds = linear_regression.predict(X_test_tfidf)\n\n# Convert predictions back to categorical format\nlr_preds_categorical = ['Good' if pred > 0 else 'Average' if pred == 0 else 'Bad' for pred in lr_preds]\n\n# Print classification reports for each model\nprint(\"Support Vector Machine:\")\nprint(classification_report(y_test, svm_preds, zero_division=1))\nprint(\"Naive Bayes:\")\nprint(classification_report(y_test, nb_preds))\nprint(\"Decision Tree:\")\nprint(classification_report(y_test, dt_preds))\nprint(\"Random Forest:\")\nprint(classification_report(y_test, rf_preds))\nprint(\"Linear Regression:\")\nprint(classification_report(y_test, lr_preds_categorical, zero_division='warn'))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:01:46.287152Z","iopub.execute_input":"2024-04-25T19:01:46.287607Z","iopub.status.idle":"2024-04-25T19:01:53.759504Z","shell.execute_reply.started":"2024-04-25T19:01:46.287569Z","shell.execute_reply":"2024-04-25T19:01:53.758609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndataset_path = \"/kaggle/input/preprocessed-reviewtext/preprocessed_headphones_reviewsText.csv\"\ndata = pd.read_csv(dataset_path)\n\n# a) Create a user-item rating matrix i.e. reviewid, asin and overall column\nmatrix = data[['reviewerID', 'asin', 'overall']]\n\n# b) Normalize the ratings, by using min-max scaling on user’s reviews\nmatrix['overall_normalized'] = (matrix['overall'] - matrix['overall'].min()) / \\\n                                         (matrix['overall'].max() - matrix['overall'].min())\n\n# Display the user-item rating matrix\nprint(\"User-Item Rating Matrix:\")\nprint(matrix)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:02:09.364236Z","iopub.execute_input":"2024-04-25T19:02:09.364627Z","iopub.status.idle":"2024-04-25T19:02:09.416046Z","shell.execute_reply.started":"2024-04-25T19:02:09.364597Z","shell.execute_reply":"2024-04-25T19:02:09.415161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pivot_matrix = matrix.pivot_table(index='reviewerID', columns='asin', values='overall_normalized', fill_value=0)\nprint(pivot_matrix)\npivot_matrix.to_csv('useritem_review_matrix.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:02:13.747992Z","iopub.execute_input":"2024-04-25T19:02:13.748353Z","iopub.status.idle":"2024-04-25T19:02:15.483238Z","shell.execute_reply.started":"2024-04-25T19:02:13.748324Z","shell.execute_reply":"2024-04-25T19:02:15.482419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#implement cosine similarity\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import KFold\n\n# Load the user-item matrix\nuser_item_matrix = pivot_matrix\n\n# Define the number of similar users to find\nN_values = [10, 20, 30, 40, 50]\n\n# Define the number of folds for K-fold validation\nK = 5\n\n# Function to calculate cosine similarity between users\ndef calculate_cosine_similarity(matrix):\n    num_users = matrix.shape[0]\n    similarity_matrix = np.zeros((num_users, num_users))\n    for i in range(num_users):\n        for j in range(i, num_users):  # Only calculate upper triangular part\n            dot_product = np.dot(matrix.iloc[i], matrix.iloc[j])\n            norm_i = np.linalg.norm(matrix.iloc[i])\n            norm_j = np.linalg.norm(matrix.iloc[j])\n            similarity = dot_product / (norm_i * norm_j) if norm_i != 0 and norm_j != 0 else 0.0\n            similarity_matrix[i][j] = np.nan_to_num(similarity)\n            similarity_matrix[j][i] = np.nan_to_num(similarity)  # Fill in the lower triangular part\n    return similarity_matrix\n\n\n\n# Function to find top N similar users for each user\ndef find_top_N_similar_users(similarity_matrix, N):\n    top_N_similar_users = {}\n    for i, row in enumerate(similarity_matrix):\n        similar_users = sorted(list(enumerate(row)), key=lambda x: x[1], reverse=True)[:N]\n        top_N_similar_users[i] = similar_users\n    return top_N_similar_users\n\n# Function to perform K-fold validation\ndef k_fold_validation(matrix, K):\n    kf = KFold(n_splits=K, shuffle=True)\n    for train_index, test_index in kf.split(matrix):\n        train_data = matrix[train_index]\n        test_data = matrix[test_index]\n        yield train_data, test_data\n\n# Calculate cosine similarity between users\nsimilarity_matrix = calculate_cosine_similarity(user_item_matrix)\n\n# Perform K-fold validation\nfor N in N_values:\n    print(f\"Top {N} Similar Users:\")\n    for fold, (train_data, test_data) in enumerate(k_fold_validation(similarity_matrix, K), 1):\n        print(f\"Fold {fold}:\")\n        # Find top N similar users for each user in the training set\n        top_N_similar_users = find_top_N_similar_users(train_data, N)\n        print(top_N_similar_users)\n        print(\"\\n\\n\")\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import KFold\n\n# Load the user-item matrix\nuser_item_matrix = pivot_matrix\n\n# Define the number of similar users to find\nN_values = [10, 20, 30, 40, 50]\n\n# Define the number of folds for K-fold validation\nK = 5\n\n# Function to calculate cosine similarity between users\ndef calculate_cosine_similarity(matrix):\n    return cosine_similarity(matrix)\n\n# Function to find top N similar users for each user\ndef find_top_N_similar_users(similarity_matrix, N):\n    top_N_similar_users = {}\n    for i, row in enumerate(similarity_matrix):\n        similar_users = sorted(list(enumerate(row)), key=lambda x: x[1], reverse=True)[:N]\n        top_N_similar_users[i] = similar_users\n    return top_N_similar_users\n\n# Function to perform K-fold validation\ndef k_fold_validation(matrix, K):\n    kf = KFold(n_splits=K, shuffle=True)\n    for train_index, test_index in kf.split(matrix):\n        train_data = matrix[train_index]\n        test_data = matrix[test_index]\n        yield train_data, test_data\n\n# Calculate cosine similarity between users\nsimilarity_matrix = calculate_cosine_similarity(user_item_matrix)\n\n# Perform K-fold validation\nfor N in N_values:\n    print(f\"Top {N} Similar Users:\")\n    for fold, (train_data, test_data) in enumerate(k_fold_validation(similarity_matrix, K), 1):\n        print(f\"Fold {fold}:\")\n        # Find top N similar users for each user in the training set\n        top_N_similar_users = find_top_N_similar_users(train_data, N)\n        print(top_N_similar_users)\n        print(\"\\n\\n\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:24:42.527390Z","iopub.execute_input":"2024-04-25T19:24:42.528238Z","iopub.status.idle":"2024-04-25T19:25:05.767115Z","shell.execute_reply.started":"2024-04-25T19:24:42.528201Z","shell.execute_reply":"2024-04-25T19:25:05.766130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import numpy as np\n\n# Calculate cosine similarity between users\ndef calculate_cosine_similarity(matrix):\n    similarity_matrix = np.zeros((matrix.shape[0], matrix.shape[0]))\n    for i in range(matrix.shape[0]):\n        for j in range(matrix.shape[0]):\n            dot_product = np.dot(matrix[i], matrix[j])\n            norm_i = np.linalg.norm(matrix[i])\n            norm_j = np.linalg.norm(matrix[j])\n            similarity_matrix[i][j] = dot_product / (norm_i * norm_j)\n    return similarity_matrix\n\n# Function to predict missing values in the user-item rating matrix\ndef predict_ratings(user_item_matrix, similarity_matrix, top_N_similar_users):\n    predicted_ratings = np.zeros(user_item_matrix.shape)\n    for user in range(user_item_matrix.shape[0]):\n        for item in range(user_item_matrix.shape[1]):\n            if user_item_matrix[user][item] == 0:  # Predict only for missing values\n                similar_users = top_N_similar_users[user]\n                numerator = 0\n                denominator = 0\n                for sim_user, sim_score in similar_users:\n                    if user_item_matrix[sim_user][item] != 0:\n                        numerator += sim_score * user_item_matrix[sim_user][item]\n                        denominator += sim_score\n                if denominator != 0:\n                    predicted_ratings[user][item] = numerator / denominator\n    return predicted_ratings\n\n# Function to calculate Mean Absolute Error\ndef calculate_mae(actual_ratings, predicted_ratings):\n    mask = actual_ratings != 0  # Mask for non-zero values\n    mae = np.mean(np.abs(actual_ratings[mask] - predicted_ratings[mask]))\n    return mae\n\n# Load the user-item matrix\nuser_item_matrix = pivot_matrix\n\n\n# Define the number of nearest neighbors to find\nN_values = [10, 20, 30, 40, 50]\n\n# Define the number of folds for K-fold validation\nK = 5\n\n# Perform K-fold validation\nfor N in N_values:\n    print(f\"Top {N} Nearest Neighbors:\")\n    total_mae = 0\n    kf = KFold(n_splits=K, shuffle=True)\n    for fold, (train_index, test_index) in enumerate(kf.split(range(user_item_matrix.shape[0])), 1):\n        train_data = user_item_matrix[train_index]\n        test_data = user_item_matrix[test_index]\n        \n        # Calculate cosine similarity between users\n        similarity_matrix = calculate_cosine_similarity(train_data)\n        \n        # Find top N nearest neighbors for each user\n        top_N_similar_users = find_top_N_similar_users(similarity_matrix, N)\n        \n        # Predict missing values in the user-item rating matrix\n        predicted_ratings = predict_ratings(train_data, similarity_matrix, top_N_similar_users)\n        \n        # Create a user-item rating matrix from the validation set\n        actual_ratings = test_data\n        \n        # Calculate Mean Absolute Error\n        fold_mae = calculate_mae(actual_ratings, predicted_ratings)\n        print(f\"Fold {fold}: MAE = {fold_mae:.4f}\")\n        total_mae += fold_mae\n    \n    avg_mae = total_mae / K\n    print(f\"Avg MAE over {K} folds = {avg_mae:.4f}\")\n    print(\"\\n\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:20:11.611648Z","iopub.status.idle":"2024-04-12T13:20:11.612213Z","shell.execute_reply.started":"2024-04-12T13:20:11.611920Z","shell.execute_reply":"2024-04-12T13:20:11.611943Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\n# Function to predict missing values using collaborative filtering\ndef predict_ratings(similarity_matrix, top_N_similar_users, train_data):\n    predicted_ratings = np.zeros_like(train_data)\n    for user, similar_users in top_N_similar_users.items():\n        for item in range(train_data.shape[1]):\n            if train_data[user, item] == 0:\n                numerator = 0\n                denominator = 0\n                for similar_user, similarity_score in similar_users:\n                    if train_data[similar_user, item] != 0:\n                        numerator += similarity_score * train_data[similar_user, item]\n                        denominator += similarity_score\n                if denominator != 0:\n                    predicted_ratings[user, item] = numerator / denominator\n    return predicted_ratings\n\n# Function to calculate MAE\ndef calculate_mae(actual_ratings, predicted_ratings):\n    actual = actual_ratings[actual_ratings.nonzero()]\n    predicted = predicted_ratings[actual_ratings.nonzero()]\n    return mean_absolute_error(actual, predicted)\n\n# Define the number of similar users to consider\nK_values = [10, 20, 30, 40, 50]\n\n# Initialize MAE dictionary to store results\nMAE_results = {}\n\n# Perform K-fold validation\nfor K in K_values:\n    print(f\"Calculating MAE for K = {K}\")\n    fold_errors = []\n    for train_data, test_data in k_fold_validation(user_item_matrix.values, K):\n        # Calculate cosine similarity between users\n        similarity_matrix = calculate_cosine_similarity(train_data)\n        \n        # Find top N similar users for each user in the training set\n        top_N_similar_users = find_top_N_similar_users(similarity_matrix, K)\n        \n        # Predict missing values in the training set\n        predicted_ratings = predict_ratings(similarity_matrix, top_N_similar_users, train_data)\n        \n        # Calculate MAE for this fold\n        fold_mae = calculate_mae(test_data, predicted_ratings)\n        fold_errors.append(fold_mae)\n    # Calculate average MAE across all folds\n    MAE_results[K] = np.mean(fold_errors)\n\n# Print MAE results\nprint(\"\\nMAE Results:\")\nfor K, MAE in MAE_results.items():\n    print(f\"K = {K}: MAE = {MAE}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:28:38.606043Z","iopub.execute_input":"2024-04-25T19:28:38.606508Z","iopub.status.idle":"2024-04-25T20:55:17.378815Z","shell.execute_reply.started":"2024-04-25T19:28:38.606471Z","shell.execute_reply":"2024-04-25T20:55:17.377779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mae_results_df = pd.DataFrame.from_dict(MAE_results, orient='index', columns=['MAE'])\nmae_results_df.index.name = 'K'\nmae_results_df.to_csv('mae_results_users.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:55:50.843308Z","iopub.execute_input":"2024-04-25T20:55:50.844184Z","iopub.status.idle":"2024-04-25T20:55:50.850516Z","shell.execute_reply.started":"2024-04-25T20:55:50.844150Z","shell.execute_reply":"2024-04-25T20:55:50.849589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Item-Item recommender system\n\n# Function to calculate cosine similarity between items\ndef calculate_item_similarity(matrix):\n    return cosine_similarity(matrix.T)\n\n# Function to find top N similar items for each item\ndef find_top_N_similar_items(similarity_matrix, N):\n    top_N_similar_items = {}\n    for i, column in enumerate(similarity_matrix.T):\n        similar_items = sorted(list(enumerate(column)), key=lambda x: x[1], reverse=True)[:N]\n        top_N_similar_items[i] = similar_items\n    return top_N_similar_items\n\n# Function to predict missing values using collaborative filtering for item-item recommendation\ndef predict_ratings_item_item(similarity_matrix, top_N_similar_items, train_data):\n    predicted_ratings = np.zeros_like(train_data)\n    for item, similar_items in top_N_similar_items.items():\n        for user in range(train_data.shape[0]):\n            if train_data[user, item] == 0:\n                numerator = 0\n                denominator = 0\n                for similar_item, similarity_score in similar_items:\n                    if train_data[user, similar_item] != 0:\n                        numerator += similarity_score * train_data[user, similar_item]\n                        denominator += similarity_score\n                if denominator != 0:\n                    predicted_ratings[user, item] = numerator / denominator\n    return predicted_ratings\n\n# Initialize MAE dictionary to store results for item-item recommendation\nMAE_results_item_item = {}\n\n# Define the number of similar items to consider\nN_values_item_item = [10, 20, 30, 40, 50]\n\n# Perform K-fold validation for item-item recommendation\nfor N_item_item in N_values_item_item:\n    print(f\"Calculating MAE for N = {N_item_item} (Item-Item)\")\n    fold_errors_item_item = []\n    for train_data, test_data in k_fold_validation(user_item_matrix.values, K):\n        # Calculate cosine similarity between items\n        item_similarity_matrix = calculate_item_similarity(train_data)\n        \n        # Find top N similar items for each item in the training set\n        top_N_similar_items_item_item = find_top_N_similar_items(item_similarity_matrix, N_item_item)\n        \n        # Predict missing values in the training set using item-item collaborative filtering\n        predicted_ratings_item_item = predict_ratings_item_item(item_similarity_matrix, top_N_similar_items_item_item, train_data)\n        \n        # Ensure test_data and predicted_ratings_item_item have the same shape\n        predicted_ratings_item_item = predicted_ratings_item_item[:test_data.shape[0], :test_data.shape[1]]\n        \n        # Calculate MAE for this fold\n        fold_mae_item_item = calculate_mae(test_data, predicted_ratings_item_item)\n        fold_errors_item_item.append(fold_mae_item_item)\n    # Calculate average MAE across all folds for item-item recommendation\n    MAE_results_item_item[N_item_item] = np.mean(fold_errors_item_item)\n\n\n# Print MAE results for item-item recommendation\nprint(\"\\nMAE Results (Item-Item):\")\nfor N_item_item, MAE_item_item in MAE_results_item_item.items():\n    print(f\"N = {N_item_item}: MAE = {MAE_item_item}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:26:55.204089Z","iopub.execute_input":"2024-04-25T21:26:55.204765Z","iopub.status.idle":"2024-04-25T23:23:13.273474Z","shell.execute_reply.started":"2024-04-25T21:26:55.204732Z","shell.execute_reply":"2024-04-25T23:23:13.272582Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Calculating MAE for N = 10 (Item-Item)\nCalculating MAE for N = 20 (Item-Item)\nCalculating MAE for N = 30 (Item-Item)\nCalculating MAE for N = 40 (Item-Item)\nCalculating MAE for N = 50 (Item-Item)\n\nMAE Results (Item-Item):\nN = 10: MAE = 0.8440111583785399\nN = 20: MAE = 0.8416532497460159\nN = 30: MAE = 0.8439610591698005\nN = 40: MAE = 0.8442460102046434\nN = 50: MAE = 0.8450043086411054\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save MAE results to a CSV file\nmae_results_df = pd.DataFrame.from_dict(MAE_results_item_item, orient='index', columns=['MAE'])\nmae_results_df.index.name = 'K'\nmae_results_df.to_csv('mae_results_items.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:23:33.646600Z","iopub.execute_input":"2024-04-25T23:23:33.646969Z","iopub.status.idle":"2024-04-25T23:23:33.653423Z","shell.execute_reply.started":"2024-04-25T23:23:33.646939Z","shell.execute_reply":"2024-04-25T23:23:33.652569Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract K values and MAE results for user-user recommender system\nK_values_user_user = list(MAE_results.keys())\nMAE_values_user_user = list(MAE_results.values())\n\n# Extract N values and MAE results for item-item recommender system\nN_values_item_item = list(MAE_results_item_item.keys())\nMAE_values_item_item = list(MAE_results_item_item.values())\n\n# Plot MAE against K for user-user recommender system\nplt.figure(figsize=(10, 6))\nplt.plot(K_values_user_user, MAE_values_user_user, marker='o', color='b', label='User-User')\nplt.title('MAE vs. K for User-User Recommender System')\nplt.xlabel('K (Number of Similar Users)')\nplt.ylabel('MAE')\nplt.xticks(K_values_user_user)\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# Plot MAE against N for item-item recommender system\nplt.figure(figsize=(10, 6))\nplt.plot(N_values_item_item, MAE_values_item_item, marker='o', color='r', label='Item-Item')\nplt.title('MAE vs. N for Item-Item Recommender System')\nplt.xlabel('N (Number of Similar Items)')\nplt.ylabel('MAE')\nplt.xticks(N_values_item_item)\nplt.grid(True)\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load MAE results for user-user recommender system from CSV\nmae_results_users = pd.read_csv('/kaggle/input/mae-results/mae_results_users.csv')\n\n# Load MAE results for item-item recommender system from CSV\nmae_results_items = pd.read_csv('/kaggle/input/mae-results/mae_results_items.csv')\n\n# Extract K values and MAE results for user-user recommender system\nK_values_user_user = mae_results_users['K'].tolist()\nMAE_values_user_user = mae_results_users['MAE'].tolist()\n\n# Extract N values and MAE results for item-item recommender system\nN_values_item_item = mae_results_items['K'].tolist()\nMAE_values_item_item = mae_results_items['MAE'].tolist()\n\n# Plot MAE against K for user-user recommender system\nplt.figure(figsize=(10, 6))\nplt.plot(K_values_user_user, MAE_values_user_user, marker='o', color='b', label='User-User')\nplt.title('MAE vs. K for User-User Recommender System')\nplt.xlabel('K (Number of Similar Users)')\nplt.ylabel('MAE')\nplt.xticks(K_values_user_user)\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# Plot MAE against N for item-item recommender system\nplt.figure(figsize=(10, 6))\nplt.plot(N_values_item_item, MAE_values_item_item, marker='o', color='r', label='Item-Item')\nplt.title('MAE vs. N for Item-Item Recommender System')\nplt.xlabel('N (Number of Similar Items)')\nplt.ylabel('MAE')\nplt.xticks(N_values_item_item)\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:05:47.849778Z","iopub.execute_input":"2024-04-25T21:05:47.850702Z","iopub.status.idle":"2024-04-25T21:05:48.433460Z","shell.execute_reply.started":"2024-04-25T21:05:47.850663Z","shell.execute_reply":"2024-04-25T21:05:48.432606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sum up the ratings for each product\nproduct_sum_ratings = matrix.groupby('asin')['overall'].sum()\n\n# Sort the products based on sum ratings in descending order\ntop_10_products = product_sum_ratings.sort_values(ascending=False).head(10)\n\n# Print the top 10 products by User Sum Ratings\nprint(\"Top 10 Products by User Sum Ratings:\")\nprint(top_10_products)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T21:12:32.849212Z","iopub.execute_input":"2024-04-11T21:12:32.849870Z","iopub.status.idle":"2024-04-11T21:12:32.862559Z","shell.execute_reply.started":"2024-04-11T21:12:32.849838Z","shell.execute_reply":"2024-04-11T21:12:32.861539Z"},"trusted":true},"execution_count":null,"outputs":[]}]}